{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "name": "M3_NB_MiniProject_3_Complex_Analytics_Pyspark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ganeshbio/CDS-B1-G9/blob/main/M3_NB_MiniProject_3_Complex_Analytics_Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "correct-ecology"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Mini-Project: Complex Analytics using Pyspark"
      ],
      "id": "correct-ecology"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "terminal-strip"
      },
      "source": [
        "## Problem Statement"
      ],
      "id": "terminal-strip"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "supported-supplement"
      },
      "source": [
        "Perform complex analytics on a network intrusion dataset using Pyspark"
      ],
      "id": "supported-supplement"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "decimal-replication"
      },
      "source": [
        "## Learning Objectives"
      ],
      "id": "decimal-replication"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "informational-yacht"
      },
      "source": [
        "At the end of the mini-project, you will be able to :\n",
        "\n",
        "* analyze the data using Pyspark\n",
        "* implement RDD based operations on the data\n",
        "* derive insights from the complex data"
      ],
      "id": "informational-yacht"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quality-approval"
      },
      "source": [
        "### Dataset"
      ],
      "id": "quality-approval"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biblical-syracuse"
      },
      "source": [
        "The dataset chosen for this mini-project is a [10% subset](https://www.kdd.org/kdd-cup/view/kdd-cup-1999/Data) of the **[KDD Cup 1999 dataset](http://kdd.ics.uci.edu/databases/kddcup99/task.html)** (Computer network intrusion detection). This is the dataset used for the Third International Knowledge Discovery and Data Mining Tools Competition. The competition task was to build a network intrusion detector, a predictive model capable of distinguishing between ``bad`` connections, called intrusions or attacks, and ``good`` normal connections. This database contains a standard set of data to be audited, which includes a wide variety of intrusions simulated in a military network environment."
      ],
      "id": "biblical-syracuse"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "listed-numbers"
      },
      "source": [
        "## Information"
      ],
      "id": "listed-numbers"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skilled-concern"
      },
      "source": [
        "Since 1999, KDD’99 has been the most widely used data set for the evaluation of anomaly detection methods. This data set is prepared by S. J. Stolfo and is built based on the data captured in DARPA’98 IDS evaluation program. DARPA’98 is about 4 gigabytes of compressed raw (binary) tcpdump data of 7 weeks of network traffic, which can be processed into about 5 million connection\n",
        "records, each with about 100 bytes. KDD dataset consists of approximately 4,900,000 single connection vectors each of which contains 41 features and is labeled as either normal or an attack, with exactly one specific attack type. The simulated attacks fall into one of the following four categories:\n",
        "\n",
        "* Denial of Service Attack (DoS): making some computing or memory resources too busy so that they deny legitimate users access to these resources.\n",
        "* User to Root Attack (U2R): unauthorized access from a remote machine according to exploit machine's vulnerabilities.\n",
        "* Remote to Local Attack (R2L): unauthorized access to local super user (root) privileges using system's susceptibility.\n",
        "* Probing Attack: host and port scans as precursors to other attacks. An attacker scans a network to gather information or find known vulnerabilities.\n",
        "\n",
        "KDD’99 features can be classified into three groups:\n",
        "\n",
        "1) Basic features: this category encapsulates all the attributes that can be extracted from a TCP/IP connection. Most of these features leading to an implicit delay in detection.\n",
        "\n",
        "2) Traffic features: this category includes features that are computed with respect to a window interval and is divided into two groups:\n",
        "\n",
        "  * \"same host\" features\n",
        "\n",
        "  * \"same service\" features\n",
        "\n",
        "3) Content features: unlike most of the DoS and Probing attacks, the R2L and U2R attacks don’t have any intrusion frequent sequential patterns. This is because the DoS and Probing attacks involve many connections to some host(s) in a very short period of time, however the R2L and U2R attacks are embedded in the data portions of the packets, and normally involve only a single connection. To detect these kinds of attacks, we need some features to be able to look for suspicious behavior in the data portion, e.g., the number of failed login attempts. These features are called content features."
      ],
      "id": "skilled-concern"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BewwTjZaJojg"
      },
      "source": [
        "## Grading = 10 Points"
      ],
      "id": "BewwTjZaJojg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "extreme-working",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53984498-f0d6-4282-8672-8808dc835d8e"
      },
      "source": [
        "#@title Install packages and Download Dataset\n",
        "!pip -qq install pyspark\n",
        "# Download the data\n",
        "!wget -qq http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\n",
        "# Download feature names\n",
        "!wget -qq https://kdd.ics.uci.edu/databases/kddcup99/kddcup.names\n",
        "print(\"Successfully Installed packages and downloaded datasets!\")"
      ],
      "id": "extreme-working",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 212.3MB 71kB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 40.2MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Successfully Installed packages and downloaded datasets!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VwMxTT0KObc"
      },
      "source": [
        "### Creating Spark Session and Load the data (1 point)"
      ],
      "id": "6VwMxTT0KObc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "italic-gross"
      },
      "source": [
        "#### Import required packages"
      ],
      "id": "italic-gross"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "published-remove"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.mllib.stat import Statistics \n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from operator import add"
      ],
      "id": "published-remove",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "proved-pressure"
      },
      "source": [
        "#### Create a Spark session\n",
        "\n",
        "Create a Spark session is a combined entry point of a Spark application, which came into implementation from Spark 2.0 (Instead of having various contexts, everything is encapsulated in a Spark session)"
      ],
      "id": "proved-pressure"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "confused-xerox",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "4a8f63a1-2342-4058-a41f-8bc25630c5a3"
      },
      "source": [
        "# Start spark session\n",
        "# YOUR CODE HERE\n",
        "spark = SparkSession.builder.appName('KDDCup').getOrCreate()\n",
        "spark"
      ],
      "id": "confused-xerox",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e807d5df7b04:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>KDDCup</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fb3a242cd10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coastal-communications"
      },
      "source": [
        "#### Creating an RDD from a File\n",
        "\n",
        "The most common way of creating an RDD is to load it from a file. Notice that Spark's textFile can handle compressed files directly."
      ],
      "id": "coastal-communications"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "accredited-venture",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3b51c34-4ac5-438e-e8f6-39e85694bac5"
      },
      "source": [
        "# YOUR CODE HERE to access sparkContext from sparkSession instance.\n",
        "rdd_name = spark.sparkContext.textFile(\"/content/kddcup.names\")\n",
        "rdd_name.collect()"
      ],
      "id": "accredited-venture",
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['back,buffer_overflow,ftp_write,guess_passwd,imap,ipsweep,land,loadmodule,multihop,neptune,nmap,normal,perl,phf,pod,portsweep,rootkit,satan,smurf,spy,teardrop,warezclient,warezmaster.',\n",
              " 'duration: continuous.',\n",
              " 'protocol_type: symbolic.',\n",
              " 'service: symbolic.',\n",
              " 'flag: symbolic.',\n",
              " 'src_bytes: continuous.',\n",
              " 'dst_bytes: continuous.',\n",
              " 'land: symbolic.',\n",
              " 'wrong_fragment: continuous.',\n",
              " 'urgent: continuous.',\n",
              " 'hot: continuous.',\n",
              " 'num_failed_logins: continuous.',\n",
              " 'logged_in: symbolic.',\n",
              " 'num_compromised: continuous.',\n",
              " 'root_shell: continuous.',\n",
              " 'su_attempted: continuous.',\n",
              " 'num_root: continuous.',\n",
              " 'num_file_creations: continuous.',\n",
              " 'num_shells: continuous.',\n",
              " 'num_access_files: continuous.',\n",
              " 'num_outbound_cmds: continuous.',\n",
              " 'is_host_login: symbolic.',\n",
              " 'is_guest_login: symbolic.',\n",
              " 'count: continuous.',\n",
              " 'srv_count: continuous.',\n",
              " 'serror_rate: continuous.',\n",
              " 'srv_serror_rate: continuous.',\n",
              " 'rerror_rate: continuous.',\n",
              " 'srv_rerror_rate: continuous.',\n",
              " 'same_srv_rate: continuous.',\n",
              " 'diff_srv_rate: continuous.',\n",
              " 'srv_diff_host_rate: continuous.',\n",
              " 'dst_host_count: continuous.',\n",
              " 'dst_host_srv_count: continuous.',\n",
              " 'dst_host_same_srv_rate: continuous.',\n",
              " 'dst_host_diff_srv_rate: continuous.',\n",
              " 'dst_host_same_src_port_rate: continuous.',\n",
              " 'dst_host_srv_diff_host_rate: continuous.',\n",
              " 'dst_host_serror_rate: continuous.',\n",
              " 'dst_host_srv_serror_rate: continuous.',\n",
              " 'dst_host_rerror_rate: continuous.',\n",
              " 'dst_host_srv_rerror_rate: continuous.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY0QylZEYTvg"
      },
      "source": [
        "Load the dataset and show the top 10 records"
      ],
      "id": "pY0QylZEYTvg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wound-repository",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02cdc0b3-b80f-4beb-eea2-49628d232667"
      },
      "source": [
        "filePath = \"/content/kddcup.data_10_percent.gz\"\n",
        "# YOUR CODE HERE\n",
        "data_rdd=spark.sparkContext.textFile(filePath)\n",
        "data_rdd.take(10)"
      ],
      "id": "wound-repository",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n",
              " '0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.',\n",
              " '0,tcp,http,SF,235,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,29,29,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n",
              " '0,tcp,http,SF,219,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,39,39,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n",
              " '0,tcp,http,SF,217,2032,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,49,49,1.00,0.00,0.02,0.00,0.00,0.00,0.00,0.00,normal.',\n",
              " '0,tcp,http,SF,217,2032,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,59,59,1.00,0.00,0.02,0.00,0.00,0.00,0.00,0.00,normal.',\n",
              " '0,tcp,http,SF,212,1940,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,2,0.00,0.00,0.00,0.00,1.00,0.00,1.00,1,69,1.00,0.00,1.00,0.04,0.00,0.00,0.00,0.00,normal.',\n",
              " '0,tcp,http,SF,159,4087,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,5,5,0.00,0.00,0.00,0.00,1.00,0.00,0.00,11,79,1.00,0.00,0.09,0.04,0.00,0.00,0.00,0.00,normal.',\n",
              " '0,tcp,http,SF,210,151,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,8,89,1.00,0.00,0.12,0.04,0.00,0.00,0.00,0.00,normal.',\n",
              " '0,tcp,http,SF,212,786,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,8,99,1.00,0.00,0.12,0.05,0.00,0.00,0.00,0.00,normal.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hollow-offense"
      },
      "source": [
        "### RDD Basic Operations (4 points)"
      ],
      "id": "hollow-offense"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "varying-amateur"
      },
      "source": [
        "#### Convert the data to CSV format (list of elements).\n",
        "\n",
        "To create a Dataframe using the RDD file, convert each row into a list by splitting with a comma (,)\n",
        "\n",
        "Hint: `map()` and `split()`"
      ],
      "id": "varying-amateur"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "designed-malta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c12776a-9c59-4be5-f7e6-aadc32f26c45"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "csv_data = data_rdd.map(lambda x: x.split(\",\"))\n",
        "csv_data.take(1)"
      ],
      "id": "designed-malta",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['0',\n",
              "  'tcp',\n",
              "  'http',\n",
              "  'SF',\n",
              "  '181',\n",
              "  '5450',\n",
              "  '0',\n",
              "  '0',\n",
              "  '0',\n",
              "  '0',\n",
              "  '0',\n",
              "  '1',\n",
              "  '0',\n",
              "  '0',\n",
              "  '0',\n",
              "  '0',\n",
              "  '0',\n",
              "  '0',\n",
              "  '0',\n",
              "  '0',\n",
              "  '0',\n",
              "  '0',\n",
              "  '8',\n",
              "  '8',\n",
              "  '0.00',\n",
              "  '0.00',\n",
              "  '0.00',\n",
              "  '0.00',\n",
              "  '1.00',\n",
              "  '0.00',\n",
              "  '0.00',\n",
              "  '9',\n",
              "  '9',\n",
              "  '1.00',\n",
              "  '0.00',\n",
              "  '0.11',\n",
              "  '0.00',\n",
              "  '0.00',\n",
              "  '0.00',\n",
              "  '0.00',\n",
              "  '0.00',\n",
              "  'normal.']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restricted-bonus"
      },
      "source": [
        "Count how many interactions are normal and attacked in the dataset.\n",
        "\n",
        "Hint: apply `filter` on each row, except the rows with 'normal.', all the remaining values are attacked."
      ],
      "id": "restricted-bonus"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "architectural-appraisal",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fba9f1f-37f6-49e6-d67b-7f1b7b755de2"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "normal_raw_data = csv_data.filter(lambda x: 'normal.' in x)\n",
        "attacked_raw_data=csv_data.filter(lambda x: 'normal.' not in x)\n",
        "normal_count=normal_raw_data.count()\n",
        "attacked_count=attacked_raw_data.count()\n",
        "\n",
        "print(\"Normal Count-->\",normal_count,\"Attack Count-->\",attacked_count,\"Total Count-->\",csv_data.count())"
      ],
      "id": "architectural-appraisal",
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normal Count--> 97278 Attack Count--> 396743 Total Count--> 494021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flying-apartment"
      },
      "source": [
        "#### Protocol and Service combinations using Cartesian product\n",
        "\n",
        "We can compute the Cartesian product between two RDDs by using the Cartesian transformation. It returns all possible pairs of elements between two RDDs. In our case, we will use it to generate all the possible combinations between Service and Protocol in our network interactions.\n",
        "\n",
        "First of all, isolate each collection of values in two separate RDDs. For that use `distinct` on the CSV-parsed dataset. From the dataset description, we know that protocol is the second column and service is the third."
      ],
      "id": "flying-apartment"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "automatic-turning",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7383ca46-6131-4e09-f580-7bd38da139da"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "protocols = csv_data.map(lambda x: x[1]).distinct()\n",
        "protocols.collect()"
      ],
      "id": "automatic-turning",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tcp', 'udp', 'icmp']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUscAdJUVGXK",
        "outputId": "d089d559-562a-4333-b7d0-1306cef6d025"
      },
      "source": [
        "services = csv_data.map(lambda x: x[2]).distinct()\n",
        "services.collect()"
      ],
      "id": "cUscAdJUVGXK",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['http',\n",
              " 'smtp',\n",
              " 'finger',\n",
              " 'domain_u',\n",
              " 'auth',\n",
              " 'telnet',\n",
              " 'ftp',\n",
              " 'eco_i',\n",
              " 'ntp_u',\n",
              " 'ecr_i',\n",
              " 'other',\n",
              " 'private',\n",
              " 'pop_3',\n",
              " 'ftp_data',\n",
              " 'rje',\n",
              " 'time',\n",
              " 'mtp',\n",
              " 'link',\n",
              " 'remote_job',\n",
              " 'gopher',\n",
              " 'ssh',\n",
              " 'name',\n",
              " 'whois',\n",
              " 'domain',\n",
              " 'login',\n",
              " 'imap4',\n",
              " 'daytime',\n",
              " 'ctf',\n",
              " 'nntp',\n",
              " 'shell',\n",
              " 'IRC',\n",
              " 'nnsp',\n",
              " 'http_443',\n",
              " 'exec',\n",
              " 'printer',\n",
              " 'efs',\n",
              " 'courier',\n",
              " 'uucp',\n",
              " 'klogin',\n",
              " 'kshell',\n",
              " 'echo',\n",
              " 'discard',\n",
              " 'systat',\n",
              " 'supdup',\n",
              " 'iso_tsap',\n",
              " 'hostnames',\n",
              " 'csnet_ns',\n",
              " 'pop_2',\n",
              " 'sunrpc',\n",
              " 'uucp_path',\n",
              " 'netbios_ns',\n",
              " 'netbios_ssn',\n",
              " 'netbios_dgm',\n",
              " 'sql_net',\n",
              " 'vmnet',\n",
              " 'bgp',\n",
              " 'Z39_50',\n",
              " 'ldap',\n",
              " 'netstat',\n",
              " 'urh_i',\n",
              " 'X11',\n",
              " 'urp_i',\n",
              " 'pm_dump',\n",
              " 'tftp_u',\n",
              " 'tim_i',\n",
              " 'red_i']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alleged-ranking"
      },
      "source": [
        "Now let's do the Cartesian product\n",
        "\n",
        "Hint: [cartesian](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.cartesian.html#:~:text=Return%20the%20Cartesian%20product%20of,and%20b%20is%20in%20other%20.)"
      ],
      "id": "alleged-ranking"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "focused-circulation",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80239bc5-480f-4013-fa05-9478a248fc99"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "product = protocols.cartesian(services).collect()\n",
        "print (\"There are {} combinations of Protocol X Service\".format(len(product)))"
      ],
      "id": "focused-circulation",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 198 combinations of protocol X service\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boolean-representation"
      },
      "source": [
        "#### Inspecting interaction duration\n",
        "\n",
        "select the total duration of interactions for normal and attack intrusion types.\n",
        "* Use the above filtered normal and attacked data and convert the duration column to integer type using `map()`\n",
        "* get the sum of duration by applying `reduce` on both the data using add operator\n",
        "* find the mean of duration by dividing the sum with count\n",
        "\n",
        "Hint: [reduce()](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.reduce.html)"
      ],
      "id": "boolean-representation"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "requested-amateur",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc2fe317-a65d-432a-c4b7-2b000f181e07"
      },
      "source": [
        "# YOUR CODE HERE\n"
      ],
      "id": "requested-amateur",
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-c4c47f2d1111>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnew_normal_duration\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcsv_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnew_normal_duration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m         \"\"\"\n\u001b[0;32m-> 1235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m         \"\"\"\n\u001b[0;32m-> 1224\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \"\"\"\n\u001b[1;32m    948\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 106.0 failed 1 times, most recent failure: Lost task 0.0 in stage 106.0 (TID 104) (e807d5df7b04 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 1235, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 1235, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-100-c4c47f2d1111>\", line 2, in <lambda>\nAttributeError: 'list' object has no attribute 'split'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor54.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 1235, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 1235, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-100-c4c47f2d1111>\", line 2, in <lambda>\nAttributeError: 'list' object has no attribute 'split'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bulgarian-confidentiality"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "bulgarian-confidentiality",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "directed-officer"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "directed-officer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tested-auction"
      },
      "source": [
        "#### Data aggregation with key/value pair RDDs\n",
        "\n",
        "We can use all the transformations and actions available for normal RDDs with key/value pair RDDs. We just need to make the functions work with pair elements.\n",
        "\n",
        "* create a key/value pair of intrusion type and duration\n",
        "* calculate the total duration of each intrusion type using `reduceByKey()`"
      ],
      "id": "tested-auction"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chicken-logging"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "chicken-logging",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "balanced-table"
      },
      "source": [
        "### Create a DataFrame with the header as features (2 points)"
      ],
      "id": "balanced-table"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-S3RgOYdghm"
      },
      "source": [
        "* Read the features (*kddcup.names*) and preprocess.\n",
        "\n",
        "    Hints:\n",
        "    - Each feature description appears row-wise in *kddcup.names*\n",
        "    - The first row consists of distinct values of intrusion_types\n",
        "    - Add or move the *intrusion_types* column name to the last, to align with the data.\n",
        "    - Each feature is represented as *feature_name*: *type*, remove *type* after colon (:)"
      ],
      "id": "Y-S3RgOYdghm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stylish-chance"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "stylish-chance",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA-HDFEIdlwd"
      },
      "source": [
        "* Create a dataframe with the data and headers as preprocessed feature names"
      ],
      "id": "XA-HDFEIdlwd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJh_-plCdlZ1"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "MJh_-plCdlZ1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "numerous-montreal"
      },
      "source": [
        "#### What is the count of each protocol type?\n",
        "\n",
        "Hint: apply `groupby` on protocol_type and count the records"
      ],
      "id": "numerous-montreal"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unusual-arizona"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "unusual-arizona",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liable-edgar"
      },
      "source": [
        "#### Register the DataFrame as a temporary table and extract the data using queries\n",
        "\n",
        "Hint: [registerTempTable](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.registerTempTable.html)"
      ],
      "id": "liable-edgar"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NA8e0kTefVr"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "_NA8e0kTefVr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWOzPDAveLmd"
      },
      "source": [
        "* query to extract the label and their frequencies\n",
        " \n",
        " Hint: `SQLContext.sql(query)`"
      ],
      "id": "PWOzPDAveLmd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thirty-automation"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "thirty-automation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ4d2Y8SeP48"
      },
      "source": [
        "* select the distinct protocol types with their count of transactions which are not normal"
      ],
      "id": "dQ4d2Y8SeP48"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "respiratory-parking"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "respiratory-parking",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGUDJ92yeSBa"
      },
      "source": [
        "* select count of transactions in each protocol type that lasts more than 1 second (duration > 1000), with no data transfer from destination (dst_bytes == 0)"
      ],
      "id": "AGUDJ92yeSBa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "conventional-driving"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "conventional-driving",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "likely-gambling"
      },
      "source": [
        "### Find the highly correlated columns (2 points)\n",
        "\n",
        "* identify the columns which are not integer type and remove those columns\n",
        "* apply correlation function on the data (Hint: `Statistics.corr()`)\n",
        "* collect the names of the columns on which correlation is applied\n",
        "* create a dataframe with correlation matrix with index and columns as names\n",
        "* get the highly correlated features by considering a correlation value greater than 0.8\n",
        "\n",
        "    Hint: `np.triu()` , `pd.mask()`"
      ],
      "id": "likely-gambling"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cooked-promise"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "cooked-promise",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "polish-claim"
      },
      "source": [
        "# eliminating names of columns having string type\n",
        "# YOUR CODE HERE"
      ],
      "id": "polish-claim",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "european-aaron"
      },
      "source": [
        "# creating a dataframe with correlation matrix\n",
        "# YOUR CODE HERE"
      ],
      "id": "european-aaron",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skilled-domestic"
      },
      "source": [
        "#  fill the null values with 0\n",
        "# YOUR CODE HERE"
      ],
      "id": "skilled-domestic",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "approved-dublin"
      },
      "source": [
        "# Finding features with correlation value more than specified threshold value (bar=0.9)\n",
        "# YOUR CODE HERE"
      ],
      "id": "approved-dublin",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "working-accident"
      },
      "source": [
        "### Analysis report (1 points)\n",
        "\n",
        "* Find the ratio of attacked transactions vs normal transactions\n",
        "\n",
        "    Hint: encode instrusion_type column by replacing normal with 1 and all other with 0\n",
        "\n",
        "* Describe the statistics of attacked and normal transactions\n",
        "    \n",
        "    Hint: Min, Max, Mean\n",
        "    \n",
        "* Select any two features that influence the intrusion_type and visualize the scatter plot to see the separation of normal and attacked"
      ],
      "id": "working-accident"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "greatest-pressing"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "greatest-pressing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dimensional-inspiration"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "dimensional-inspiration",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csNSaN2oaNYS"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "id": "csNSaN2oaNYS",
      "execution_count": null,
      "outputs": []
    }
  ]
}