{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M5_NB_MiniProject_2_Face_Mask_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ganeshbio/CDS-B1-G9/blob/main/M5_NB_MiniProject_2_Face_Mask_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "another-optimum"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "\n",
        "##  A program by IISc and TalentSprint\n",
        "\n",
        "### Mini Project Notebook: Face Mask Detection using Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maritime-miami"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nljJR6CwfZN_"
      },
      "source": [
        "At the end of the miniproject, you will be able to :\n",
        "\n",
        "* load and extract features of images using ImageDataGenerator\n",
        "\n",
        "* build the convolutional neural networks\n",
        "\n",
        "* use the pre-trained models using keras applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29152de7"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This project uses a Deep Neural Network, more specifically a Convolutional Neural Network, to differentiate between images of people, with masks, without masks and incorrectly placed masks. Manually built and pretrained networks will be used to perform this classification task.\n",
        "\n",
        "**Face-Mask-Detection-Using-CNN**\n",
        "\n",
        "* Outbreak of the Coronavirus pandemic has created various changes in the lifestyle of everyone around the world.\n",
        "* Among these changes, wearing a mask has been very vital to every individual.\n",
        "* Detection of people who are not wearing masks is a challenge due to the large populations.\n",
        "* This face mask detection project can be used in schools, hospitals, banks, airports etc as a digitalized scanning tool. \n",
        "  - The technique of detecting peopleâ€™s faces and segregating them into three classes namely the people with masks and people without masks and partial masks is done with the help of image processing and deep learning.\n",
        "* With the help of this project, a person who is monitoring the face mask status for a particular firm can be seated in a remote area and still monitor efficiently and give instructions accordingly.\n",
        "\n",
        "![img](https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2020/07/dataset-with-mask-1536x844.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "surprising-uruguay"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The data for this mini-project is collected from various sources including the masked images from internet and general frontal face images considered as without mask. This dataset consists of 5029 train images and 1059 test images with 3 classes `with_mask`, `without_mask` and `partial_mask`\n",
        "\n",
        "Many people are not correctly wearing their masks due to bad practices, bad behaviors or vulnerability of individuals (e.g., children, old people). For these reasons, several mask wearing campaigns intend to sensitize people about this problem and good practices. In this sense, this work proposes three types of masked face detection dataset; namely, the Correctly Masked Face, the Incorrectly Masked Face and their combination for the global masked face detection. This dataset serves the objective of classifying faces that are: \n",
        "  \n",
        "- Without Mask/ With Mask/ Partial Mask\n",
        "  \n",
        "Note that this dataset contains some annotated (artificially generated) masks to augment the 'masked' data category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih-oasWmdZul"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfWGmjNHdZul"
      },
      "source": [
        "To build and implement a Convolutional Neural Network model to classify between masked/unmasked/partially masked faces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "operating-latter"
      },
      "source": [
        "## Grading = 10 Points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "812a816f",
        "cellView": "form"
      },
      "source": [
        "#@title Download the data\n",
        "!wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/MP2_FaceMask_Dataset.zip\n",
        "!unzip -qq MP2_FaceMask_Dataset.zip\n",
        "print(\"Data Downloaded Successfuly!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abstract-stocks"
      },
      "source": [
        "### Import Required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG52PDGENRgN"
      },
      "source": [
        "import numpy as np\n",
        "import PIL\n",
        "from matplotlib import pyplot as plt\n",
        "import glob, os\n",
        "from tensorflow import keras\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense,Dropout\n",
        "from keras.models import Model, load_model\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53g0zVbjRV7K"
      },
      "source": [
        "## Data Loading and preprocessing (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYSjwlcSGJq1"
      },
      "source": [
        "### Analyze the shape of images and distribution of classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_FC0knCfeFD"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN1ZYJL8Ts0Q"
      },
      "source": [
        "### Load the images using ImageDataGenerator\n",
        "\n",
        "There are two main steps involved in creating the generator.\n",
        "1. Instantiate ImageDataGenerator with required arguments to create an object\n",
        "2. Use the `flow_from_directory` command depending on how your data is stored on disk. This is the command that will allow you to generate and get access to batches of data on the fly.\n",
        "\n",
        "Hint: [link](https://keras.io/api/preprocessing/image/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqsC5w4f5NOr"
      },
      "source": [
        "TRAINING_DIR = \"/content/MP2_FaceMask_Dataset/train/\"\n",
        "VALIDATION_DIR = \"/content/MP2_FaceMask_Dataset/test/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBmLkk-zNeZs"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdQY-dzZeqWm"
      },
      "source": [
        "### Visualize the sample images of each class using data generator\n",
        "\n",
        "Hint: plt.subplot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCgAcqdGewsb"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkOr1nhNT5yD"
      },
      "source": [
        "## Build the CNN model using Keras (4 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1GGjQTfTIoO"
      },
      "source": [
        "**Convolutional Neural Network:** A neural network in which at least one layer is a convolutional layer. A typical convolutional neural network consists of some combination of the following layers:\n",
        "\n",
        "* convolutional layers\n",
        "* pooling layers\n",
        "* dense layers\n",
        "\n",
        "\n",
        "**Conv2D**  \n",
        "\n",
        "Passing an image with input shape of 3-D and to calculate the output: \n",
        "\n",
        " $O = \\frac{n - f + 2p}{s} + 1$\n",
        "\n",
        "**MaxPool** \n",
        "\n",
        "The resulting output, when using the \"valid\" padding option, has a spatial shape (number of rows or columns) of: \n",
        "\n",
        "O = `math.floor`$(\\frac{input shape - pool size)}{ strides}) + 1$ (when input shape >= pool size)\n",
        "\n",
        "The resulting output shape when using the \"same\" padding option is: \n",
        "\n",
        "O = `math.floor`$(\\frac{input shape - 1}{strides}) + 1$\n",
        "\n",
        "by default, stride = None, so stride is same as pool size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmGNu31vmLhs"
      },
      "source": [
        "Task-flow\n",
        "* Initialize the network of convolution, maxpooling and dense layers\n",
        "* Define the optimizer and loss functions\n",
        "* Fit the model and evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdaobFneNaJO"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFPKELTRnv9s"
      },
      "source": [
        "## Transfer learning (4 points)\n",
        "\n",
        "Transfer learning consists of taking features learned on one problem, and leveraging them on a new, similar problem.\n",
        "\n",
        "A pre-trained model is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task.\n",
        "\n",
        "The intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset.\n",
        "\n",
        "For eg. Using VGG16, we remove the last layer which takes a probability for each of the 1000 classes in the ImageNet and replaces it with a layer that takes 3 probabilities in our case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lMUVOj0UdAi"
      },
      "source": [
        "### Use the pre-trained models ([VGG16](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/VGG16) or [ResNet50](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50))\n",
        "\n",
        "* Load the pre-trained model\n",
        "* Fit and evaluate the data\n",
        "\n",
        "Hint: [How to use pre-trained model](https://towardsdatascience.com/step-by-step-guide-to-using-pretrained-models-in-keras-c9097b647b29)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YI0gnq-DLyu"
      },
      "source": [
        "#### Expected accuracy: More than 90%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4GX9CwFqOXm"
      },
      "source": [
        "Task-flow\n",
        "* Initialize the network with the weights of Imagenet\n",
        "* Fine tune the network by modifying fully connected layers.\n",
        "* Re-train the model with our problem data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx2Z2vLZqXm0"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4tF71VIUwzK"
      },
      "source": [
        "### Capture the live image using the below code cell and predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "I6kTLnTmrcCg"
      },
      "source": [
        "#@title Capture the photo\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename\n",
        "\n",
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  print(str(err))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNe9vDQ6q6Z6"
      },
      "source": [
        "After executing above cell and capturing the photo, load the captured photo and predict with model.\n",
        "\n",
        "**Note:** \n",
        "* Convert the image to numpy array and resize to the shape which model accept. \n",
        "* Extend the dimension (to 4-D shape) of an image, as the model is trained on a batch of inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD5gr9YOAX83"
      },
      "source": [
        "features = PIL.Image.open(\"photo.jpg\")\n",
        "plt.imshow(features);\n",
        "# YOUR CODE HERE to predict the image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBIA1sbLByIL"
      },
      "source": [
        "### Report Analysis\n",
        "\n",
        "- Compare the accuracies for the Pre-trained vs CNN models\n",
        "- What process was followed to tune the hyperparameters?\n",
        "- Which model detects the mask/no mask/ partial mask more accurately with the live pictures?\n",
        "- Discuss the confusion matrix in terms of the misclassifications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKnc1WZk9cIk"
      },
      "source": [
        "### Instructions for preparing Kaggle competition predictions\n",
        "\n",
        "* Load the image paths from Kaggle testset using `glob`\n",
        "* Read the Images using `PIL.Image.open` and resize to required shape.\n",
        "* Get the predictions using trained model and prepare a csv file\n",
        "  - FC layer of DeepNet model gives output for each class, consider the maximum value among all classes as prediction using `np.argmax`.\n",
        "* Predictions (csv) file should contain 2 columns as Sample_Submission.csv\n",
        "  - First column is the img_path which is considered as index\n",
        "  - Second column is prediction in decoded form (for eg. with_mask, partial_mask, without_mask).\n",
        "\n",
        "Refer [Kaggle Team Creation Instructions Doc](https://drive.google.com/file/d/1ZhCs4VHkXT2HLITL5ZJlZbgUSFIeWYrU/view?usp=sharing) for more information."
      ]
    }
  ]
}